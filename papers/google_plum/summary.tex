\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\begin{document}

\section*{PLUM: Adapting Pre-trained Language Models for Industrial-scale Generative Recommendations (Google, 2025)}

\subsection*{Challenges}
\begin{itemize}
  \item \textbf{Embedding table bottleneck}: Traditional large embedding models (LEMs) in industrial recommenders rely on massive sparse embedding tables, leading to scaling limits (memory, training inefficiency, and deployment cost) as vocabulary and behavioral signals grow.
  \item \textbf{Semantic ID quality}: Generative retrieval depends on high-quality Semantic IDs (SIDs). Prior SID generation methods are limited by (i) single-modality content encoders and (ii) weak alignment to user co-occurrence signals.
  \item \textbf{LLM alignment for recommendations}: Pre-trained LLMs have rich sequence modeling ability but are not aligned to recommendation tasks or SID modalities. The challenge is aligning LLMs with item/token spaces while preserving generalization.
  \item \textbf{Industrial-scale constraints}: The system must handle large-scale YouTube recommendation traffic with strict latency, throughput, and retrieval quality constraints.
\end{itemize}

\subsection*{Key Initiatives (Core Contributions)}
\begin{itemize}
  \item \textbf{PLUM framework}: A full pipeline to adapt pre-trained LLMs for industrial-scale generative recommendation, combining semantic ID generation, continued pre-training, and task-specific fine-tuning.
  \item \textbf{SIDv2 enhancements}: A redesigned SID generation pipeline with (i) fused multi-modal content representations, (ii) multi-resolution codebooks, (iii) progressive masking for hierarchy, and (iv) contrastive co-occurrence regularization.
  \item \textbf{Continued Pre-Training (CPT)}: Large-scale CPT on user behavior + domain text to align SIDs as a new modality and ground them in language.
  \item \textbf{LLM-based generative retrieval}: Autoregressively generate item SIDs to retrieve items, replacing large embedding tables.
  \item \textbf{Scaling study}: Iso-FLOPS experiments to study compute-optimal scaling, showing synchronized scaling of model size and training data is required.
\end{itemize}

\subsection*{Methodology}
\paragraph{Overall Pipeline}
\begin{enumerate}
  \item \textbf{SID generation (SIDv2)}: Create high-quality SIDs for items from multi-modal content and collaborative signals.
  \item \textbf{Continued Pre-Training (CPT)}: Align the LLM with SID modality + user behavior sequences.
  \item \textbf{Supervised Fine-Tuning (SFT)}: Train the LLM for generative retrieval by predicting next-item SIDs in user sequences.
\end{enumerate}

\paragraph{SIDv2 Details}
\begin{itemize}
  \item \textbf{Fused multi-modal representation}: Combine textual metadata, visual content, and audio features to represent items; addresses single-modality limitations.
  \item \textbf{Multi-resolution codebooks}: Codebook cardinality decreases with quantization level (e.g., $2048/2^{\ell-1}$) to keep earlier levels high-resolution and later levels compact.
  \item \textbf{Progressive masking}: Randomly mask higher-level codebooks during training to enforce hierarchical SID structure and interpretability.
  \item \textbf{Co-occurrence contrastive regularization}: Add a contrastive loss term in RQ-VAE training to bring together items frequently co-watched, injecting collaborative filtering signals into SID space.
\end{itemize}

\paragraph{PLUM Training}
\begin{itemize}
  \item \textbf{CPT}: Pre-train LLM on large-scale sequences of user behaviors with SIDs plus domain text to align modalities and ground semantic meaning.
  \item \textbf{SFT}: Train autoregressive generation of next-item SID in user sequence; model acts as a generative retriever.
\end{itemize}

\subsection*{Experiments}
\begin{itemize}
  \item \textbf{Datasets}: Large-scale internal YouTube recommendation datasets (video interactions).
  \item \textbf{Baselines}: Heavily-optimized production model built with large embedding tables (LEMs) and other SID-based or hybrid retrieval systems.
  \item \textbf{Main Results}: PLUM substantially improves retrieval performance (e.g., Recall@10) compared to embedding-table-based production baselines, while eliminating massive embedding tables.
  \item \textbf{Ablations}: Validate the benefit of (i) CPT stage, (ii) LLM initialization, (iii) SIDv2 techniques (multi-resolution codebooks, progressive masking, co-occurrence contrastive loss).
  \item \textbf{Scaling Studies}: Iso-FLOPS experiments show that compute-optimal training requires synchronized scaling of both model size and training data; performance continues improving up to MoE models with $>900$M activated parameters.
  \item \textbf{Industrial Deployment}: PLUM was deployed in YouTube retrieval, demonstrating practical feasibility and production impact.
\end{itemize}

\subsection*{References (selected)}
\begin{itemize}
  \item Hoffmann et al. (2022). Training compute-optimal large language models. arXiv:2203.15556.
  \item Hou et al. (2024). Large Language Models are Zero-Shot Rankers for Recommender Systems. (Springer).
  \item Huang et al. (2025). Towards Large-scale Generative Ranking. arXiv:2505.04180.
  \item Ju et al. (2025). Generative Recommendation with Semantic IDs: A Practitionerâ€™s Handbook. arXiv:2507.22224.
  \item Kang \& McAuley (2018). Self-Attentive Sequential Recommendation. arXiv:1808.09781.
  \item Li et al. (2025). BBQRec: Behavior-Bind Quantization for Multi-Modal Sequential Recommendation. arXiv:2504.06636.
  \item Liu et al. (2022). Monolith: Real Time Recommendation System With Collisionless Embedding Table. arXiv:2209.07663.
\end{itemize}

\end{document}
